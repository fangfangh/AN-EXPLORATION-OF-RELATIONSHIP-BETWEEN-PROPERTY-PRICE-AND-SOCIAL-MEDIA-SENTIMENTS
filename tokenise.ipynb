{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import  WordNetLemmatizer \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import json\n",
    " \n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "#abbrevs={\"won't\":\"will not\",\"i'm\":\"i am\",\"don't\":\"do not\",\"doesn't\":\"does not\",\"isn't\":\"is not\",\"i'll\":\"i will\"}\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['RT','rt','VIA', 'via']\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(s,arra):\n",
    "    tokens = preprocess(s)\n",
    "    count = 0\n",
    "    count2 =0\n",
    "    str = []\n",
    "    for token in tokens:\n",
    "        #canonicalization of text\n",
    "        value =lemmatiser.lemmatize(token, pos=\"v\")\n",
    "        value1 =lemmatiser.lemmatize(value)\n",
    "        value2 = value1.encode('utf8') \n",
    "        #converting the entire text into lowercase\n",
    "        value2 = value2.lower() \n",
    "        #expanding abbreviations\n",
    "        \"\"\"\n",
    "        for abbrev in abbrevs:\n",
    "            value2= value2.replace(abbrev,abbrevs[abbrev])\n",
    "        \"\"\"\n",
    "        pattern = re.compile('http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+' )\n",
    "        if not pattern.match(value2):\n",
    "            if value2 not in arra: \n",
    "                str.append(value2)\n",
    "        \n",
    "    \n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def removestopwords(s):\n",
    "    #remove punctuations\n",
    "    \n",
    "    \n",
    "    \n",
    "    return s\n",
    "def preprocess(s, lowercase=False): \n",
    "    tokens = tokenize(s) \n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEW', 'LISTING', 'Inspect', 'Saturday', '11', ':', '30', '-', '12', ':', '15', 'pm', '#pottspoint', '#centennialpark', '#realestate', '#property', '#studio', '#forsale', 'https://t.co/OaLmD3AHOq']\n",
      "['\\\\', 'u2022', 'NEW', 'LISTING', '\\\\', 'u2022', 'Inspect', 'Saturday', '11', ':', '30', '-', '12', ':', '15', 'pm', '#pottspoint', '#centennialpark', '#realestate', '#property', '#studio', '#forsale', 'https://t.co/OaLmD3AHOq']\n"
     ]
    }
   ],
   "source": [
    "text = \"\\u2022 NEW LISTING \\u2022\\nInspect Saturday 11:30 - 12:15pm\\n#pottspoint #centennialpark #realestate #property #studio #forsale https://t.co/OaLmD3AHOq\"\n",
    "tweet=text.decode('unicode_escape').encode('ascii','ignore')\n",
    "print(preprocess(tweet))\n",
    "print(preprocess(text))\n",
    "#print(normalization(preprocess(tweet))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all = Counter()\n",
    "count_bigram = Counter()\n",
    "f = open(\"property-view-perth.json\", 'r')\n",
    "lines = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = normalization(lines.decode('unicode_escape').encode('ascii','ignore'),stop)\n",
    "\n",
    "count_all.update(token)\n",
    " \n",
    "terms = normalization(lines.decode('unicode_escape').encode('ascii','ignore'),punctuation)\n",
    "terms_bigram = bigrams(terms)\n",
    "count_bigram.update(terms_bigram)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('115.859', 2467), ('31.9522', 2467), ('perth', 2116), ('realestate', 2040), ('perthisok', 1251), ('home', 1232), ('house', 909), ('westernaustralia', 844), ('newhomesperth', 731), ('australia', 684), ('perthproperty', 683), ('build', 633), ('happiness', 619), ('forsale', 592), ('perthrealestate', 590), ('happy', 583), ('dogsofinstagram', 542), ('dog', 542), ('perthlife', 522), ('moretocelebrate', 508), ('legend', 508), ('helper', 508)]\n"
     ]
    }
   ],
   "source": [
    "print(count_all.most_common(22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('31.9522', '115.859'), 2467), (('team', '140,'), 138), (('wao', '138,'), 138), (('115.859', 'realestate'), 118), (('perth', 'realestate'), 115), (('realestate', '31.9522'), 102), (('perth', '31.9522'), 97), (('interior', '123,'), 95), (('realestate', 'perth'), 91), (('115.859', 'perth'), 87), (('31.9947', '115.86'), 64), (('31.95511', '115.86285'), 61), (('115.859', 'perthisok'), 61), (('31.94665', '115.86113'), 54), (('1000', 'thpost'), 53), (('31.9728298', '115.8260193'), 53), (('31.95495', '115.82361'), 53), (('perthisok', 'perth'), 52), (('westernaustralia', '31.9522'), 52), (('realestate', 'perthproperty'), 50)]\n"
     ]
    }
   ],
   "source": [
    "print(count_bigram.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
